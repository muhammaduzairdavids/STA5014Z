---
title: "1. Correlation and Simple Linear Regression"
author: "Muhammad Uzair Davids"
date: "2025-02-19"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Correlation

**Correlation (*r - sample correlation coefficient*)** is the measure of strength of *linear* relationships. It ranges between -1 and 1. When *r* is 1, this indicates a strong positive relationship, and when it is -1, this indicates a strong negative relationship. When *r* is 0, this means that there is *NO LINEAR* relationship, but it does not necessarily mean that there is no relationship - there may be a strong relationship, but if that relationship isn't linear, the *Pearson correlation coefficient (sample correlation coefficient)* won't be useful.

The correlation coefficient can be very sensitive to outliers.

*CORRELATION DOES NOT IMPLY CAUSATION*

**Sample correlation (*p - Greek letter rho*)** is an estimate of the *true/population* correlation. 

Begin by loading the data:

```{r load data}
bag <- read.csv("C:/Program Files/Git/STA5014Z/data/bagasse.csv") # read the .csv and create a variable name
bag # display data
attach(bag) # allows R to access database by simply giving the variable name
```

## Scatter plots

Scatter plots are important are important tools for understanding the complete relationship between variables, whether linear or non-linear.

```{r scatter plot}
plot(calories ~ water.content, xlab = "water content (%)",
     ylab = "calories (kJ/g)", las = 1, pch = 16, data = bag)
```

The above scatter plot shows a negative linear relationship, this means that a correlation coefficient can be used for the data.  

## Test of significance

On its own, *correlation coefficients can be difficult to interpret*, since *it does not measure something we can understand*. So, we can do a *test of significance (`cor.test()`)*, which will attain the **p-value**.

The **p-value** allows us to measure the evidence against the *null hypothesis (**H0 : rho = 0**)*. However, this test is dependent on sample size, and can only indicate whether there is some evidence for a relationship between variables. There are better ways (*R-squared and linear regression*) to understand the relationship between variables. If there is a *clear response variable* then it is better to *investigate how the response changes when the explanatory variable using regression*.

Calculate the correlation coefficient between variables:

```{r correlation}
cor(calories, water.content) # calculate correlation coefficient 

cor.test(calories, water.content) # test of significance
```
The above correlation coefficient *r = -0.99* indicates a very strong negative linear relationship between calories and water content - when water content increases, calories decrease. 

The **small *p-value* (p < 0.05)** indicates **strong evidence against the null hypothesis**. 

So we know that there water content and calories are correlated, but we don't know how. What is the rate of decrease in calorific content with increase water content? How much of the variability in calorific value is explained by water content? To get these answers, we need to use *linear regression*. 

# Simple Linear Regression

Simple linear regression involves fitting a line to our observed data. This line **describes the relationship between the response and the explanatory variable in the form of the following equation:** 

*y = a + bx* OR response = a + b(predictor)

The line is an **estimate of the true relationship with uncertainty**. Regression is used to **describe and understand relationships between variables**, **estimate parameters**, **estimate the response for explanatory variables not directly observed** and to **understand variability**. 

Create the model using the following code:
`model <- lm(reponse variable ~ predictor variable)`

```{r linear regression model}
model1 <- lm(calories ~ water.content) # Create the model
summary(model1) # Summarises model 
```
### *1. Interpret the coefficient estimates*

The estimate of the intercept (Beta 0) is 19.16, and the slope estimate (Beta 1) is -0.19. So our regression equation is: 

*y = 19.16 - 0.19 x `water.content`* (y should be written with a hat to denote an estimate of y/the calorific value in this case).

The above equation tells us that if water content increases by 1%, the average calorific value decreases by 0.19kJ/g, and this rate of change is constant since it is linear. The intercept at 19.16, is the average calorific value when `water.content` = 0.

The coefficients also have **standard errors**. This is a **measure of precision of the estimate**. The more information we have, the better the precision. A **small standard error** means that our estimate is close to the true value, and vice versa.

### *2. Hypothesis test* 

*t-value* is the test statistic in a *t-test*, and it **measures how far the observed value is from 0 (H0 = TRUE)**. If H0 is true, then **t-value < 2**.

The *p-value* (`Pr(>|t|)`) is a **measure of this distance between the observed value compared to the null hypothesis**. A **small p-value (p < 0.05)** indicates **strong evidence against H0**, and vice versa.

The p and t values help to check if there is evidence in the data for a relationship. Relationships NOT supported by data should NOT be interpreted. 

## Model checking

```{r checking our model}
ml <- lm(y ~ x)
summary(ml)
```

